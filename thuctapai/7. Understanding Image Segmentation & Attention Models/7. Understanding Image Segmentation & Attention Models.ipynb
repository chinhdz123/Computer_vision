{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcfa0b2",
   "metadata": {},
   "source": [
    "<h3>Segmentation<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a33ec",
   "metadata": {},
   "source": [
    "Segmentation gồm 3 loại:\n",
    "   * Semantic: tất cả pixel trong ảnh được phân loại nhãn\n",
    "   * Instance: Phân vùng đối tượng và phân loại nhãn cho đối tượng đó\n",
    "   * Panoptic: kết hợp 2 cái trên"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb2c277",
   "metadata": {},
   "source": [
    "![segmentation](segmentation.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc2db3",
   "metadata": {},
   "source": [
    "<h3>Attention Models<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c5165",
   "metadata": {},
   "source": [
    "Trong nhiều trường hợp, chúng ta thấy mạng neuron truyền thống không có khả năng hoạt động với lượng dữ liệu lớn, thêm layer có thể giúp mạng neural ghi nhớ các dữ liệu lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d39190",
   "metadata": {},
   "source": [
    "Nếu chúng ta cung cấp một tập dữ liệu để model học, có thể có một vài thông tin quan trọng bị bỏ qua. Chú ý tới thông tin quan trọng là cần thiết và nó có thể cải thiện hiệu suất của model. Nó có thể đạt được bằng cách thêm một tính năng chú ý bổ sung. Mạng neuron xây dựng bằng nhiều lớp khác nhau có thể dễ dàng kết hợp tính năng này qua một lớp. Chúng ta có thể dùng lớp bổ sung trong cấu hình của nó để cải thiện hiệu suất.\n",
    "Trong bài này, chúng ta sẽ đề cập tới lớp chú ý trong mạng neuron và hiểu tầm quan trọng và làm như thế nào để thêm nó vào mạng trong thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2c27c",
   "metadata": {},
   "source": [
    "___Cơ chế Attention___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b5711",
   "metadata": {},
   "source": [
    "Tất cả pixel trong ảnh đầu vào được dùng để dự đoán đầu ra mặc dù không phải phần nào của bức ảnh cũng quan trọng như nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c44787",
   "metadata": {},
   "source": [
    "![bird](bird.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dbd537",
   "metadata": {},
   "source": [
    "Attention models chỉ tập trung vào các phần quan trọng, giảm khối lượng tính toán và giúp ích rất trong việc xử lý lượng dữ liệu lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b49d99",
   "metadata": {},
   "source": [
    "Tập trung vào các phần quan trọng và xử lý từng phần một."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67e3ee",
   "metadata": {},
   "source": [
    "![Capture](Capture.JPG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
